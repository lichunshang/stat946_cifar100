/home/cs4li/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
train data shape: (50000, 32, 32, 3)
50000 train samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 300)       8400      
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 300)       0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 300)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 600)       720600    
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 600)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 600)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 8, 8, 900)         2160900   
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 900)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 4, 4, 900)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 1200)        4321200   
_________________________________________________________________
dropout_4 (Dropout)          (None, 4, 4, 1200)        0         
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 2, 2, 1200)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 1500)        7201500   
_________________________________________________________________
dropout_5 (Dropout)          (None, 2, 2, 1500)        0         
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 1, 1, 1500)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 1, 1, 1800)        10801800  
_________________________________________________________________
dropout_6 (Dropout)          (None, 1, 1, 1800)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1800)              0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 1800)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 100)               180100    
=================================================================
Total params: 25,394,500
Trainable params: 25,394,500
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
2018-02-14 14:20:04.140952: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-14 14:20:04.229015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-14 14:20:04.229468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.31GiB
2018-02-14 14:20:04.229501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)
2018-02-14 14:20:05.595525: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.42GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:06.079442: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:06.858834: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.83GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:08.449518: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.87GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:08.799261: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:09.046850: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.99GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:09.943164: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.66GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:09.943218: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.74GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:10.329006: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-02-14 14:20:10.329060: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.66GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
195/195 [==============================] - 259s 1s/step - loss: 4.4367 - acc: 0.0202
Epoch 2/10
195/195 [==============================] - 258s 1s/step - loss: 4.0406 - acc: 0.0574
Epoch 3/10
195/195 [==============================] - 248s 1s/step - loss: 3.7769 - acc: 0.0989
Epoch 4/10
195/195 [==============================] - 248s 1s/step - loss: 3.5355 - acc: 0.1403
Epoch 5/10
195/195 [==============================] - 247s 1s/step - loss: 3.3435 - acc: 0.1802
Epoch 6/10
195/195 [==============================] - 248s 1s/step - loss: 3.1523 - acc: 0.2134
Epoch 7/10
195/195 [==============================] - 247s 1s/step - loss: 2.9961 - acc: 0.2422
Epoch 8/10
195/195 [==============================] - 247s 1s/step - loss: 2.8632 - acc: 0.2674
Epoch 9/10
195/195 [==============================] - 247s 1s/step - loss: 2.7484 - acc: 0.2928
Epoch 10/10
195/195 [==============================] - 247s 1s/step - loss: 2.6111 - acc: 0.3216
